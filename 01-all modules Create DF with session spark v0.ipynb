{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a77a7664",
      "metadata": {
        "id": "a77a7664"
      },
      "outputs": [],
      "source": [
        "#import pyspark\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16623655",
      "metadata": {
        "id": "16623655"
      },
      "source": [
        "Spark – Default interface for Scala and Java\n",
        "\n",
        "PySpark – Python interface for Spark\n",
        "\n",
        "SparklyR – R interface for Spark.\n",
        "\n",
        "\n",
        "Modules & packages:\n",
        "\n",
        "- PySpark RDD (pyspark.RDD)\n",
        "\n",
        "- PySpark DataFrame and SQL (pyspark.sql)\n",
        "\n",
        "- PySpark Streaming (pyspark.streaming)\n",
        "\n",
        "- PySpark MLib (pyspark.ml, pyspark.mllib)\n",
        "\n",
        "- PySpark GraphFrames (GraphFrames)\n",
        "\n",
        "- PySpark Resource (pyspark.resource) It’s new in PySpark 3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e15252ed",
      "metadata": {
        "id": "e15252ed"
      },
      "source": [
        "- SparkSession can be created using a builder() or newSession() methods of the SparkSession.\n",
        "\n",
        "Spark session internally creates a sparkContext variable of SparkContext. \n",
        "\n",
        "You can create multiple SparkSession objects but only one SparkContext per JVM. \n",
        "\n",
        "- In case if you want to create another new SparkContext you should stop existing Sparkcontext (using stop()) before creating a new one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ad733df",
      "metadata": {
        "id": "5ad733df",
        "outputId": "ebd4a940-a027-45f6-d470-a16a098533d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/nestor/opt/anaconda3/envs/PySpark/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/01/19 17:44:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "# create spark session\n",
        "spark = SparkSession.builder.appName('SparkSession1').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e186bb7",
      "metadata": {
        "id": "5e186bb7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create DataFrame in PySpark Shell\n",
        "data = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000),('R',80000)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a57cd55d",
      "metadata": {
        "id": "a57cd55d"
      },
      "source": [
        "- By using createDataFrame() function of the SparkSession you can create a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e434722",
      "metadata": {
        "id": "7e434722",
        "outputId": "6a556882-9cc4-4797-f9b1-acf0a1212567"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------+\n",
            "|    _1|    _2|\n",
            "+------+------+\n",
            "|  Java| 20000|\n",
            "|Python|100000|\n",
            "| Scala|  3000|\n",
            "|     R| 80000|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.createDataFrame(data)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62bab998",
      "metadata": {
        "id": "62bab998",
        "outputId": "5f5a8758-ffa5-4bb7-c2e8-fe030612cd6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(_1='Java', _2=20000)]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b7d2ec8",
      "metadata": {
        "id": "0b7d2ec8",
        "outputId": "cc29ceb5-52fa-428c-c533-e0163e5fd13e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['_1', '_2']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d1567a5",
      "metadata": {
        "id": "3d1567a5",
        "outputId": "963cfeb3-7ffd-472f-ed96-6195afea290a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f305f2",
      "metadata": {
        "id": "95f305f2",
        "outputId": "660c04e8-0e82-446b-d415-1c00c6004a12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('_1', 'string'), ('_2', 'bigint')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f48b3abf",
      "metadata": {
        "id": "f48b3abf",
        "outputId": "e990ca56-488e-4496-ace3-94d7eae33a4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Column 1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumnRenamed('_1','Column 1').printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e95e73a8",
      "metadata": {
        "id": "e95e73a8",
        "outputId": "d49cd95e-c266-42bf-e3a9-a74c14b7f82a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Column 1: string (nullable = true)\n",
            " |-- Column 2: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumnRenamed('_1','Column 1').withColumnRenamed('_2','Column 2').printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c11a1c4f",
      "metadata": {
        "id": "c11a1c4f"
      },
      "outputs": [],
      "source": [
        "df2 = df.withColumnRenamed('_1','Column 1').withColumnRenamed('_2','Column 2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52fb500d",
      "metadata": {
        "id": "52fb500d",
        "outputId": "b0ccc514-0ede-4645-a420-fddf2bc72f80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Column 1: string (nullable = true)\n",
            " |-- Column 2: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1ec891",
      "metadata": {
        "id": "9d1ec891",
        "outputId": "f02a7c19-2a58-4662-8326-a2a064ce08d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------+\n",
            "|Column 1|Column 2|\n",
            "+--------+--------+\n",
            "|    Java|   20000|\n",
            "|  Python|  100000|\n",
            "|   Scala|    3000|\n",
            "|       R|   80000|\n",
            "+--------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ed23883",
      "metadata": {
        "id": "7ed23883"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec9e8a4",
      "metadata": {
        "id": "eec9e8a4",
        "outputId": "cbf363e2-49be-4b3d-b47b-3fe06f509c8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# define the columns names\n",
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "\n",
        "# create Df form data and columns (shema)\n",
        "\n",
        "df3 = spark.createDataFrame(data=data, schema = columns)\n",
        "df3.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa51a49e",
      "metadata": {
        "id": "aa51a49e"
      },
      "source": [
        "DataFrames are created from external sources like files from:\n",
        "\n",
        "the local system, HDFS, \n",
        "S3 Azure, HBase, \n",
        "MySQL table e.t.c. \n",
        "\n",
        "Below is an example of how to read a CSV file from a local system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed76b087",
      "metadata": {
        "id": "ed76b087"
      },
      "outputs": [],
      "source": [
        "# create spark session\n",
        "spark = SparkSession.builder.appName('SparkforSql').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eac59ff",
      "metadata": {
        "id": "0eac59ff"
      },
      "outputs": [],
      "source": [
        "\n",
        "df4 = spark.read.option(\"header\",True) \\\n",
        "    .csv(\"country_codes.csv\")\n",
        "\n",
        "\n",
        "#df = spark.read.option(\"header\",True) \\\n",
        "#     .csv(\"/tmp/resources/zipcodes.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b231f789",
      "metadata": {
        "id": "b231f789",
        "outputId": "179306d4-cd87-4d9a-a821-6739e77499da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+------------+------------+-------+\n",
            "|            Country|Alpha-2 code|Alpha-3 code|Numeric|\n",
            "+-------------------+------------+------------+-------+\n",
            "|        Afghanistan|          AF|         AFG|      4|\n",
            "|            Albania|          AL|         ALB|      8|\n",
            "|            Algeria|          DZ|         DZA|     12|\n",
            "|     American Samoa|          AS|         ASM|     16|\n",
            "|            Andorra|          AD|         AND|     20|\n",
            "|             Angola|          AO|         AGO|     24|\n",
            "|           Anguilla|          AI|         AIA|    660|\n",
            "|         Antarctica|          AQ|         ATA|     10|\n",
            "|Antigua and Barbuda|          AG|         ATG|     28|\n",
            "|          Argentina|          AR|         ARG|     32|\n",
            "|            Armenia|          AM|         ARM|     51|\n",
            "|              Aruba|          AW|         ABW|    533|\n",
            "|          Australia|          AU|         AUS|     36|\n",
            "|            Austria|          AT|         AUT|     40|\n",
            "|         Azerbaijan|          AZ|         AZE|     31|\n",
            "|      Bahamas (the)|          BS|         BHS|     44|\n",
            "|            Bahrain|          BH|         BHR|     48|\n",
            "|         Bangladesh|          BD|         BGD|     50|\n",
            "|           Barbados|          BB|         BRB|     52|\n",
            "|            Belarus|          BY|         BLR|    112|\n",
            "+-------------------+------------+------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df4.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df466184",
      "metadata": {
        "id": "df466184",
        "outputId": "0be8bac4-8423-45ad-8f05-31d690a3e8b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('James', ['Java', 'Scala'], {'hair': 'black', 'eye': 'brown'}),\n",
              " ('Michael', ['Spark', 'Java', None], {'hair': 'brown', 'eye': None}),\n",
              " ('Robert', ['CSharp', ''], {'hair': 'red', 'eye': ''}),\n",
              " ('Washington', None, None),\n",
              " ('Jefferson', ['1', '2'], {})]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# array data\n",
        "arrayData = [\n",
        "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
        "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
        "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
        "        ('Washington',None,None),\n",
        "        ('Jefferson',['1','2'],{})\n",
        "        ]\n",
        "\n",
        "arrayData\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d512e9",
      "metadata": {
        "id": "f5d512e9",
        "outputId": "2c60a660-dffe-4545-ebbb-3da5b7881ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- knownLanguages: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+----------+-------------------+--------------------+\n",
            "|      name|     knownLanguages|          properties|\n",
            "+----------+-------------------+--------------------+\n",
            "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
            "|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n",
            "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
            "|Washington|               null|                null|\n",
            "| Jefferson|             [1, 2]|                  {}|\n",
            "+----------+-------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df5 = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
        "df5.printSchema()\n",
        "df5.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b830a1c7",
      "metadata": {
        "id": "b830a1c7"
      },
      "outputs": [],
      "source": [
        "# use SQL, first, \n",
        "# create a temporary table on DataFrame \n",
        "# using createOrReplaceTempView() function.\n",
        "\n",
        "df5.createOrReplaceTempView(\"PERSON_Lenguages\")  # previous DF\n",
        "df6 = spark.sql(\"SELECT * from PERSON_Lenguages\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6318fe9",
      "metadata": {
        "id": "a6318fe9",
        "outputId": "6a9d2594-1026-41f4-c559-6635be518d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- knownLanguages: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+----------+-------------------+--------------------+\n",
            "|      name|     knownLanguages|          properties|\n",
            "+----------+-------------------+--------------------+\n",
            "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
            "|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n",
            "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
            "|Washington|               null|                null|\n",
            "| Jefferson|             [1, 2]|                  {}|\n",
            "+----------+-------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df6.printSchema()\n",
        "df6.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40734f2e",
      "metadata": {
        "id": "40734f2e",
        "outputId": "e6258b75-9d8e-43a3-8787-5706a5006eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# new data\n",
        "\n",
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "\n",
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "df7 = spark.createDataFrame(data=data, schema = columns)\n",
        "df7.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c20a0c87",
      "metadata": {
        "id": "c20a0c87",
        "outputId": "ca844965-8a9c-40c0-e4b7-46269dd12ee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n",
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# using createOrReplaceTempView() function.\n",
        "\n",
        "df7.createOrReplaceTempView(\"PERSON_DATA\")  # previous DF\n",
        "df9 = spark.sql(\"SELECT * from PERSON_DATA\")\n",
        "df9.show()\n",
        "df9.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed91c0f4",
      "metadata": {
        "id": "ed91c0f4",
        "outputId": "0993276d-932a-4ad6-ff30-6b05960eb991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------------------+--------------------+\n",
            "|      name|     knownLanguages|          properties|\n",
            "+----------+-------------------+--------------------+\n",
            "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
            "|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n",
            "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
            "|Washington|               null|                null|\n",
            "| Jefferson|             [1, 2]|                  {}|\n",
            "+----------+-------------------+--------------------+\n",
            "\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# have two  temporary table on DataFrame \n",
        "df6.show()\n",
        "df9.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e75ce9a5",
      "metadata": {
        "id": "e75ce9a5",
        "outputId": "5ff8adcd-7ae3-40b6-9f98-9e1c0a4d79a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------+\n",
            "|gender|count(1)|\n",
            "+------+--------+\n",
            "|     M|       3|\n",
            "|     F|       2|\n",
            "+------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "groupDF = spark.sql(\"SELECT gender, count(*) from PERSON_DATA group by gender\")\n",
        "groupDF.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "960ce287",
      "metadata": {
        "id": "960ce287",
        "outputId": "99c56af5-3a4f-418b-b1bd-4efcb6b0714e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|     knownLanguages|\n",
            "+-------------------+\n",
            "|      [Java, Scala]|\n",
            "|[Spark, Java, null]|\n",
            "|         [CSharp, ]|\n",
            "|               null|\n",
            "|             [1, 2]|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "groupDFL = spark.sql(\"SELECT  knownLanguages from PERSON_Lenguages\")\n",
        "groupDFL.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71b8c0d5",
      "metadata": {
        "id": "71b8c0d5",
        "outputId": "c04e2c3c-7afd-4468-b876-bd35a9c62d15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 49:>                                                         (0 + 8) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------+\n",
            "|gender|count(1)|\n",
            "+------+--------+\n",
            "|     M|       3|\n",
            "|     F|       2|\n",
            "+------+--------+\n",
            "\n",
            "root\n",
            " |-- gender: string (nullable = true)\n",
            " |-- count(1): long (nullable = false)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "groupDF.show()\n",
        "groupDF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "661b193c",
      "metadata": {
        "id": "661b193c",
        "outputId": "5bf40837-2d99-46be-daba-e85135770146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|     knownLanguages|\n",
            "+-------------------+\n",
            "|      [Java, Scala]|\n",
            "|[Spark, Java, null]|\n",
            "|         [CSharp, ]|\n",
            "|               null|\n",
            "|             [1, 2]|\n",
            "+-------------------+\n",
            "\n",
            "root\n",
            " |-- knownLanguages: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "groupDFL.show()\n",
        "groupDFL.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6767bd7",
      "metadata": {
        "id": "a6767bd7"
      },
      "source": [
        "Add months"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82fb9924",
      "metadata": {
        "id": "82fb9924"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "author SparkByExamples.com\n",
        "\"\"\"\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkMonth').getOrCreate()\n",
        "\n",
        "from pyspark.sql.functions import col,expr\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c032c26c",
      "metadata": {
        "id": "c032c26c",
        "outputId": "975d2289-0082-4918-c2fd-08e442761608"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('2019-01-23', 1), ('2019-06-24', 2), ('2019-09-20', 3)]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef571eb8",
      "metadata": {
        "id": "ef571eb8",
        "outputId": "98aa890f-07ec-4984-dd40-96a4483f8d83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---------+----------+\n",
            "|      date|increment|  inc_date|\n",
            "+----------+---------+----------+\n",
            "|2019-01-23|        1|2019-02-23|\n",
            "|2019-06-24|        2|2019-08-24|\n",
            "|2019-09-20|        3|2019-12-20|\n",
            "+----------+---------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.createDataFrame(data).toDF(\"date\",\"increment\") \\\n",
        "    .select(col(\"date\"),col(\"increment\"), \\\n",
        "      expr(\"add_months(to_date(date,'yyyy-MM-dd'),cast(increment as int))\").alias(\"inc_date\")) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cb3153b",
      "metadata": {
        "id": "3cb3153b"
      },
      "source": [
        "Add new columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "929ef85a",
      "metadata": {
        "id": "929ef85a",
        "outputId": "760c794a-3e90-46f2-d17c-bc68d11d3605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+------+------+\n",
            "|firstname|lastname|gender|salary|\n",
            "+---------+--------+------+------+\n",
            "|    James|   Smith|     M|  3000|\n",
            "|     Anna|    Rose|     F|  4100|\n",
            "|   Robert|Williams|     M|  6200|\n",
            "+---------+--------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data = [('James','Smith','M',3000),\n",
        "  ('Anna','Rose','F',4100),\n",
        "  ('Robert','Williams','M',6200), \n",
        "]\n",
        "\n",
        "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a17fc38c",
      "metadata": {
        "id": "a17fc38c",
        "outputId": "e48e09a6-92b0-4104-cb2a-d40a355d6230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aa\n",
            "+---------+--------+------+------+-------------+\n",
            "|firstname|lastname|gender|salary|bonus_percent|\n",
            "+---------+--------+------+------+-------------+\n",
            "|    James|   Smith|     M|  3000|          0.3|\n",
            "|     Anna|    Rose|     F|  4100|          0.3|\n",
            "|   Robert|Williams|     M|  6200|          0.3|\n",
            "+---------+--------+------+------+-------------+\n",
            "\n",
            "+---------+--------+------+------+------------+\n",
            "|firstname|lastname|gender|salary|bonus_amount|\n",
            "+---------+--------+------+------+------------+\n",
            "|    James|   Smith|     M|  3000|       900.0|\n",
            "|     Anna|    Rose|     F|  4100|      1230.0|\n",
            "|   Robert|Williams|     M|  6200|      1860.0|\n",
            "+---------+--------+------+------+------------+\n",
            "\n",
            "+---------+--------+------+------+---------------+\n",
            "|firstname|lastname|gender|salary|           name|\n",
            "+---------+--------+------+------+---------------+\n",
            "|    James|   Smith|     M|  3000|    James,Smith|\n",
            "|     Anna|    Rose|     F|  4100|      Anna,Rose|\n",
            "|   Robert|Williams|     M|  6200|Robert,Williams|\n",
            "+---------+--------+------+------+---------------+\n",
            "\n",
            "+---------+--------+------+------+------------+\n",
            "|firstname|lastname|gender|salary|current_date|\n",
            "+---------+--------+------+------+------------+\n",
            "|    James|   Smith|     M|  3000|  2023-01-19|\n",
            "|     Anna|    Rose|     F|  4100|  2023-01-19|\n",
            "|   Robert|Williams|     M|  6200|  2023-01-19|\n",
            "+---------+--------+------+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if 'salary1' not in df.columns:\n",
        "    print(\"aa\")\n",
        "    \n",
        "# Add new constanct column\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "df.withColumn(\"bonus_percent\", lit(0.3)) \\\n",
        "  .show()\n",
        "  \n",
        "#Add column from existing column\n",
        "df.withColumn(\"bonus_amount\", df.salary*0.3) \\\n",
        "  .show()\n",
        "\n",
        "#Add column by concatinating existing columns\n",
        "from pyspark.sql.functions import concat_ws\n",
        "\n",
        "df.withColumn(\"name\", concat_ws(\",\",\"firstname\",'lastname')) \\\n",
        "  .show()\n",
        "\n",
        "\n",
        "#Add current date\n",
        "from pyspark.sql.functions import current_date\n",
        "\n",
        "df.withColumn(\"current_date\", current_date()) \\\n",
        "  .show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd175e8",
      "metadata": {
        "id": "fcd175e8",
        "outputId": "cf2f2be4-0bad-4096-e288-c23512df9983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+------+------+-----+\n",
            "|firstname|lastname|gender|salary|grade|\n",
            "+---------+--------+------+------+-----+\n",
            "|    James|   Smith|     M|  3000|    A|\n",
            "|     Anna|    Rose|     F|  4100|    B|\n",
            "|   Robert|Williams|     M|  6200|    C|\n",
            "+---------+--------+------+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "df.withColumn(\"grade\", \\\n",
        "   when((df.salary < 4000), lit(\"A\")) \\\n",
        "     .when((df.salary >= 4000) & (df.salary <= 5000), lit(\"B\")) \\\n",
        "     .otherwise(lit(\"C\")) \\\n",
        "  ).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26f7216a",
      "metadata": {
        "id": "26f7216a",
        "outputId": "c58d0323-8080-4dd7-f44f-6d76bc570a77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+------+-----+\n",
            "|firstname|salary|bonus|\n",
            "+---------+------+-----+\n",
            "|    James|  3000|  0.3|\n",
            "|     Anna|  4100|  0.3|\n",
            "|   Robert|  6200|  0.3|\n",
            "+---------+------+-----+\n",
            "\n",
            "+---------+------+------------+\n",
            "|firstname|salary|bonus_amount|\n",
            "+---------+------+------------+\n",
            "|    James|  3000|       900.0|\n",
            "|     Anna|  4100|      1230.0|\n",
            "|   Robert|  6200|      1860.0|\n",
            "+---------+------+------------+\n",
            "\n",
            "+---------+------+----------+\n",
            "|firstname|salary|today_date|\n",
            "+---------+------+----------+\n",
            "|    James|  3000|2023-01-19|\n",
            "|     Anna|  4100|2023-01-19|\n",
            "|   Robert|  6200|2023-01-19|\n",
            "+---------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Add column using select\n",
        "df.select(\"firstname\",\"salary\", lit(0.3).alias(\"bonus\")).show()\n",
        "df.select(\"firstname\",\"salary\", lit(df.salary * 0.3).alias(\"bonus_amount\")).show()\n",
        "df.select(\"firstname\",\"salary\", current_date().alias(\"today_date\")).show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2855e9e1",
      "metadata": {
        "id": "2855e9e1",
        "outputId": "8235121e-44a9-4219-8912-dc7362dc590c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+------+-----+\n",
            "|firstname|salary|bonus|\n",
            "+---------+------+-----+\n",
            "|    James|  3000|  0.3|\n",
            "|     Anna|  4100|  0.3|\n",
            "|   Robert|  6200|  0.3|\n",
            "+---------+------+-----+\n",
            "\n",
            "+---------+------+------------+\n",
            "|firstname|salary|bonus_amount|\n",
            "+---------+------+------------+\n",
            "|    James|  3000|       900.0|\n",
            "|     Anna|  4100|      1230.0|\n",
            "|   Robert|  6200|      1860.0|\n",
            "+---------+------+------------+\n",
            "\n",
            "+---------+------+----------+\n",
            "|firstname|salary|today_date|\n",
            "+---------+------+----------+\n",
            "|    James|  3000|2023-01-19|\n",
            "|     Anna|  4100|2023-01-19|\n",
            "|   Robert|  6200|2023-01-19|\n",
            "+---------+------+----------+\n",
            "\n",
            "+---------+------+-----+\n",
            "|firstname|salary|grade|\n",
            "+---------+------+-----+\n",
            "|    James|  3000|    B|\n",
            "|     Anna|  4100|    B|\n",
            "|   Robert|  6200|    B|\n",
            "+---------+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Add columns using SQL\n",
        "df.createOrReplaceTempView(\"PER\") # temporary table\n",
        "\n",
        "spark.sql(\"select firstname,salary, '0.3' as bonus from PER\").show()\n",
        "spark.sql(\"select firstname,salary, salary * 0.3 as bonus_amount from PER\").show()\n",
        "spark.sql(\"select firstname,salary, current_date() as today_date from PER\").show()\n",
        "spark.sql(\"select firstname,salary, \" +\n",
        "          \"case salary when salary < 4000 then 'A' \"+\n",
        "          \"else 'B' END as grade from PER\").show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37cf7089",
      "metadata": {
        "id": "37cf7089",
        "outputId": "6609fd96-26af-4a02-f062-606c9d0f7141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+------+------+\n",
            "|firstname|lastname|gender|salary|\n",
            "+---------+--------+------+------+\n",
            "|    James|   Smith|     M|  3000|\n",
            "|     Anna|    Rose|     F|  4100|\n",
            "|   Robert|Williams|     M|  6200|\n",
            "+---------+--------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select * from PER\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd2eeab7",
      "metadata": {
        "id": "cd2eeab7",
        "outputId": "9edbfb34-3c76-4dc3-e1e3-379c98e8c387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aa\n",
            "+---------+--------+------+------+-------------+\n",
            "|firstname|lastname|gender|salary|bonus_percent|\n",
            "+---------+--------+------+------+-------------+\n",
            "|    James|   Smith|     M|  3000|          0.3|\n",
            "|     Anna|    Rose|     F|  4100|          0.3|\n",
            "|   Robert|Williams|     M|  6200|          0.3|\n",
            "+---------+--------+------+------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if 'salary1' not in df.columns:\n",
        "    print(\"aa\")\n",
        "    \n",
        "# Add new constanct column\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "dfM = df.withColumn(\"bonus_percent\", lit(0.3))\n",
        "dfM.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "153e744e",
      "metadata": {
        "id": "153e744e",
        "outputId": "bc0802a1-e4ae-4556-e922-9ea8d71d5d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+------+------+-------------+------------+\n",
            "|firstname|lastname|gender|salary|bonus_percent|bonus_amount|\n",
            "+---------+--------+------+------+-------------+------------+\n",
            "|    James|   Smith|     M|  3000|          0.3|       900.0|\n",
            "|     Anna|    Rose|     F|  4100|          0.3|      1230.0|\n",
            "|   Robert|Williams|     M|  6200|          0.3|      1860.0|\n",
            "+---------+--------+------+------+-------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Add column from existing column\n",
        "dfM = dfM.withColumn(\"bonus_amount\", df.salary*0.3)\n",
        "dfM.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d30f451a",
      "metadata": {
        "id": "d30f451a",
        "outputId": "6967056a-5954-4d8c-8199-73ee103c77cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+------+------+-------------+------------+---------------+------------+\n",
            "|firstname|lastname|gender|salary|bonus_percent|bonus_amount|           name|current_date|\n",
            "+---------+--------+------+------+-------------+------------+---------------+------------+\n",
            "|    James|   Smith|     M|  3000|          0.3|       900.0|    James,Smith|  2023-01-19|\n",
            "|     Anna|    Rose|     F|  4100|          0.3|      1230.0|      Anna,Rose|  2023-01-19|\n",
            "|   Robert|Williams|     M|  6200|          0.3|      1860.0|Robert,Williams|  2023-01-19|\n",
            "+---------+--------+------+------+-------------+------------+---------------+------------+\n",
            "\n",
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            " |-- bonus_percent: double (nullable = false)\n",
            " |-- bonus_amount: double (nullable = true)\n",
            " |-- name: string (nullable = false)\n",
            " |-- current_date: date (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Add column by concatinating existing columns\n",
        "from pyspark.sql.functions import concat_ws\n",
        "\n",
        "dfM = dfM.withColumn(\"name\", concat_ws(\",\",\"firstname\",'lastname')) \n",
        "\n",
        "\n",
        "#Add current date\n",
        "from pyspark.sql.functions import current_date\n",
        "\n",
        "dfM = dfM.withColumn(\"current_date\", current_date())\n",
        "\n",
        "dfM.show()\n",
        "dfM.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f32e8dd",
      "metadata": {
        "id": "4f32e8dd",
        "outputId": "3e23c02f-891b-4fd6-d253-be27139a69e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+------+------+-------------+------------+---------------+------------+-----+\n",
            "|firstname|lastname|gender|salary|bonus_percent|bonus_amount|           name|current_date|grade|\n",
            "+---------+--------+------+------+-------------+------------+---------------+------------+-----+\n",
            "|    James|   Smith|     M|  3000|          0.3|       900.0|    James,Smith|  2023-01-19|    A|\n",
            "|     Anna|    Rose|     F|  4100|          0.3|      1230.0|      Anna,Rose|  2023-01-19|    B|\n",
            "|   Robert|Williams|     M|  6200|          0.3|      1860.0|Robert,Williams|  2023-01-19|    C|\n",
            "+---------+--------+------+------+-------------+------------+---------------+------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "dfM = dfM.withColumn(\"grade\", \\\n",
        "   when((df.salary < 4000), lit(\"A\")) \\\n",
        "     .when((df.salary >= 4000) & (df.salary <= 5000), lit(\"B\")) \\\n",
        "     .otherwise(lit(\"C\")) \\\n",
        "  )\n",
        "\n",
        "dfM.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b69cc7e",
      "metadata": {
        "id": "5b69cc7e"
      },
      "source": [
        "Drop columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a02cafd",
      "metadata": {
        "id": "6a02cafd"
      },
      "outputs": [],
      "source": [
        "dfM = dfM.drop('firstname')\n",
        "dfM = dfM.drop('lastname')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "340432bf",
      "metadata": {
        "id": "340432bf",
        "outputId": "0f74ba04-8d8f-4377-d56f-8624572816c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------+-------------+------------+---------------+------------+-----+\n",
            "|gender|salary|bonus_percent|bonus_amount|           name|current_date|grade|\n",
            "+------+------+-------------+------------+---------------+------------+-----+\n",
            "|     M|  3000|          0.3|       900.0|    James,Smith|  2023-01-19|    A|\n",
            "|     F|  4100|          0.3|      1230.0|      Anna,Rose|  2023-01-19|    B|\n",
            "|     M|  6200|          0.3|      1860.0|Robert,Williams|  2023-01-19|    C|\n",
            "+------+------+-------------+------------+---------------+------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfM.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f43ffd4",
      "metadata": {
        "id": "4f43ffd4"
      },
      "source": [
        "Aggregate functions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a49888a",
      "metadata": {
        "id": "1a49888a",
        "outputId": "68e8a316-8f83-451e-c0f4-1bd3daee24b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|        James|     Sales|  3000|\n",
            "|      Michael|     Sales|  4600|\n",
            "|       Robert|     Sales|  4100|\n",
            "|        Maria|   Finance|  3000|\n",
            "|        James|     Sales|  3000|\n",
            "|        Scott|   Finance|  3300|\n",
            "|          Jen|   Finance|  3900|\n",
            "|         Jeff| Marketing|  3000|\n",
            "|        Kumar| Marketing|  2000|\n",
            "|         Saif|     Sales|  4100|\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#import pyspark\n",
        "#from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
        "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
        "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
        "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
        "from pyspark.sql.functions import variance,var_samp,  var_pop\n",
        "\n",
        "#spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "simpleData = [(\"James\", \"Sales\", 3000),\n",
        "    (\"Michael\", \"Sales\", 4600),\n",
        "    (\"Robert\", \"Sales\", 4100),\n",
        "    (\"Maria\", \"Finance\", 3000),\n",
        "    (\"James\", \"Sales\", 3000),\n",
        "    (\"Scott\", \"Finance\", 3300),\n",
        "    (\"Jen\", \"Finance\", 3900),\n",
        "    (\"Jeff\", \"Marketing\", 3000),\n",
        "    (\"Kumar\", \"Marketing\", 2000),\n",
        "    (\"Saif\", \"Sales\", 4100)\n",
        "  ]\n",
        "schema = [\"employee_name\", \"department\", \"salary\"]\n",
        "  \n",
        "  \n",
        "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
        "df.printSchema()\n",
        "#df.show(truncate=False)\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cc9fbe2",
      "metadata": {
        "id": "1cc9fbe2",
        "outputId": "3286d54e-35d3-483f-a3b8-70259556cba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "approx_count_distinct: 6\n",
            "avg: 3400.0\n",
            "+------------------------------------------------------------+\n",
            "|collect_list(salary)                                        |\n",
            "+------------------------------------------------------------+\n",
            "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
            "+------------------------------------------------------------+\n",
            "\n",
            "+------------------------------------+\n",
            "|collect_set(salary)                 |\n",
            "+------------------------------------+\n",
            "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
            "+------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"approx_count_distinct: \" + \\\n",
        "      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n",
        "\n",
        "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n",
        "\n",
        "df.select(collect_list(\"salary\")).show(truncate=False) # all\n",
        "\n",
        "df.select(collect_set(\"salary\")).show(truncate=False) # distint\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37c1e946",
      "metadata": {
        "id": "37c1e946",
        "outputId": "d393c57a-16fa-4329-8fa2-c8c67723e0fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------+\n",
            "|count(DISTINCT department, salary)|\n",
            "+----------------------------------+\n",
            "|8                                 |\n",
            "+----------------------------------+\n",
            "\n",
            "Distinct Count of Department &amp; Salary: [Row(count(DISTINCT department, salary)=8)]\n",
            "count: 10\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
        "df2.show(truncate=False)\n",
        "\n",
        "print(\"Distinct Count of Department &amp; Salary: \"+str(df2.collect()))\n",
        "\n",
        "print(\"count: \"+str(df.select(count(\"salary\")).collect()[0][0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21966cf5",
      "metadata": {
        "id": "21966cf5",
        "outputId": "f5056b61-0dbb-4077-ad19-20e5def5fd10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+\n",
            "|first(salary)|\n",
            "+-------------+\n",
            "|3000         |\n",
            "+-------------+\n",
            "\n",
            "+------------+\n",
            "|last(salary)|\n",
            "+------------+\n",
            "|4100        |\n",
            "+------------+\n",
            "\n",
            "+-------------------+\n",
            "|kurtosis(salary)   |\n",
            "+-------------------+\n",
            "|-0.6467803030303032|\n",
            "+-------------------+\n",
            "\n",
            "+-----------+\n",
            "|max(salary)|\n",
            "+-----------+\n",
            "|4600       |\n",
            "+-----------+\n",
            "\n",
            "+-----------+\n",
            "|min(salary)|\n",
            "+-----------+\n",
            "|2000       |\n",
            "+-----------+\n",
            "\n",
            "+-----------+\n",
            "|avg(salary)|\n",
            "+-----------+\n",
            "|3400.0     |\n",
            "+-----------+\n",
            "\n",
            "+--------------------+\n",
            "|skewness(salary)    |\n",
            "+--------------------+\n",
            "|-0.12041791181069571|\n",
            "+--------------------+\n",
            "\n",
            "+-------------------+-------------------+------------------+\n",
            "|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n",
            "+-------------------+-------------------+------------------+\n",
            "|765.9416862050705  |765.9416862050705  |726.636084983398  |\n",
            "+-------------------+-------------------+------------------+\n",
            "\n",
            "+-----------+\n",
            "|sum(salary)|\n",
            "+-----------+\n",
            "|34000      |\n",
            "+-----------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nestor/opt/anaconda3/envs/PySpark/lib/python3.9/site-packages/pyspark/sql/functions.py:214: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
            "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|sum(DISTINCT salary)|\n",
            "+--------------------+\n",
            "|20900               |\n",
            "+--------------------+\n",
            "\n",
            "+-----------------+-----------------+---------------+\n",
            "|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n",
            "+-----------------+-----------------+---------------+\n",
            "|586666.6666666666|586666.6666666666|528000.0       |\n",
            "+-----------------+-----------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(first(\"salary\")).show(truncate=False)\n",
        "df.select(last(\"salary\")).show(truncate=False)\n",
        "df.select(kurtosis(\"salary\")).show(truncate=False)\n",
        "df.select(max(\"salary\")).show(truncate=False)\n",
        "df.select(min(\"salary\")).show(truncate=False)\n",
        "df.select(mean(\"salary\")).show(truncate=False)\n",
        "df.select(skewness(\"salary\")).show(truncate=False)\n",
        "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n",
        "    stddev_pop(\"salary\")).show(truncate=False)\n",
        "df.select(sum(\"salary\")).show(truncate=False)\n",
        "df.select(sumDistinct(\"salary\")).show(truncate=False)\n",
        "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n",
        "  .show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bab1bca",
      "metadata": {
        "id": "1bab1bca"
      },
      "source": [
        "Order by - Group by"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50145799",
      "metadata": {
        "id": "50145799",
        "outputId": "e9aa47d9-7411-414e-8117-67365745a8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- bonus: long (nullable = true)\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NV   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |DE   |99000 |40 |24000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |NV   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NJ   |91000 |50 |21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "author SparkByExamples.com\n",
        "\"\"\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col,sum,avg,max\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "                    .appName('SparkByExamples.com') \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
        "    (\"Michael\",\"Sales\",\"NV\",86000,56,20000),\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
        "    (\"Raman\",\"Finance\",\"DE\",99000,40,24000),\n",
        "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
        "    (\"Jeff\",\"Marketing\",\"NV\",80000,25,18000),\n",
        "    (\"Kumar\",\"Marketing\",\"NJ\",91000,50,21000)\n",
        "  ]\n",
        "\n",
        "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dbecdf7",
      "metadata": {
        "id": "4dbecdf7",
        "outputId": "c79c2858-42af-4264-8d2b-ee78b45ac253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
            "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
            "|      Michael|     Sales|   NV| 86000| 56|20000|\n",
            "|         Jeff| Marketing|   NV| 80000| 25|18000|\n",
            "|        Kumar| Marketing|   NJ| 91000| 50|21000|\n",
            "|        Raman|   Finance|   DE| 99000| 40|24000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfSort=df.sort(df.state,df.salary, ascending=False)\n",
        "\n",
        "dfSort.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b267520e",
      "metadata": {
        "id": "b267520e",
        "outputId": "efff1c54-d150-4845-dc87-a0cf1c488fa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----------+\n",
            "|state|sum(salary)|\n",
            "+-----+-----------+\n",
            "|   NY|     252000|\n",
            "|   NV|     166000|\n",
            "|   CA|     171000|\n",
            "|   DE|      99000|\n",
            "|   NJ|      91000|\n",
            "+-----+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfSort=dfSort.groupBy(df.state).agg(sum(df.salary))\n",
        "dfSort.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "265369be",
      "metadata": {
        "id": "265369be"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77a7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16623655",
   "metadata": {},
   "source": [
    "Spark – Default interface for Scala and Java\n",
    "\n",
    "PySpark – Python interface for Spark\n",
    "\n",
    "SparklyR – R interface for Spark.\n",
    "\n",
    "\n",
    "Modules & packages:\n",
    "\n",
    "- PySpark RDD (pyspark.RDD)\n",
    "\n",
    "- PySpark DataFrame and SQL (pyspark.sql)\n",
    "\n",
    "- PySpark Streaming (pyspark.streaming)\n",
    "\n",
    "- PySpark MLib (pyspark.ml, pyspark.mllib)\n",
    "\n",
    "- PySpark GraphFrames (GraphFrames)\n",
    "\n",
    "- PySpark Resource (pyspark.resource) It’s new in PySpark 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15252ed",
   "metadata": {},
   "source": [
    "- SparkSession can be created using a builder() or newSession() methods of the SparkSession.\n",
    "\n",
    "Spark session internally creates a sparkContext variable of SparkContext. \n",
    "\n",
    "You can create multiple SparkSession objects but only one SparkContext per JVM. \n",
    "\n",
    "- In case if you want to create another new SparkContext you should stop existing Sparkcontext (using stop()) before creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad733df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/nestor/opt/anaconda3/envs/PySpark/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/19 17:44:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create spark session\n",
    "spark = SparkSession.builder.appName('SparkSession1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e186bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataFrame in PySpark Shell\n",
    "data = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000),('R',80000)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57cd55d",
   "metadata": {},
   "source": [
    "- By using createDataFrame() function of the SparkSession you can create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e434722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|    _1|    _2|\n",
      "+------+------+\n",
      "|  Java| 20000|\n",
      "|Python|100000|\n",
      "| Scala|  3000|\n",
      "|     R| 80000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62bab998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Java', _2=20000)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b7d2ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_1', '_2']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d1567a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f305f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_1', 'string'), ('_2', 'bigint')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48b3abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Column 1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('_1','Column 1').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e95e73a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Column 1: string (nullable = true)\n",
      " |-- Column 2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('_1','Column 1').withColumnRenamed('_2','Column 2').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c11a1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumnRenamed('_1','Column 1').withColumnRenamed('_2','Column 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52fb500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Column 1: string (nullable = true)\n",
      " |-- Column 2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d1ec891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|Column 1|Column 2|\n",
      "+--------+--------+\n",
      "|    Java|   20000|\n",
      "|  Python|  100000|\n",
      "|   Scala|    3000|\n",
      "|       R|   80000|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ed23883",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eec9e8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the columns names\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "# create Df form data and columns (shema)\n",
    "\n",
    "df3 = spark.createDataFrame(data=data, schema = columns)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51a49e",
   "metadata": {},
   "source": [
    "DataFrames are created from external sources like files from:\n",
    "\n",
    "the local system, HDFS, \n",
    "S3 Azure, HBase, \n",
    "MySQL table e.t.c. \n",
    "\n",
    "Below is an example of how to read a CSV file from a local system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed76b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession.builder.appName('SparkforSql').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eac59ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df4 = spark.read.option(\"header\",True) \\\n",
    "    .csv(\"country_codes.csv\")\n",
    "\n",
    "\n",
    "#df = spark.read.option(\"header\",True) \\\n",
    "#     .csv(\"/tmp/resources/zipcodes.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b231f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+------------+-------+\n",
      "|            Country|Alpha-2 code|Alpha-3 code|Numeric|\n",
      "+-------------------+------------+------------+-------+\n",
      "|        Afghanistan|          AF|         AFG|      4|\n",
      "|            Albania|          AL|         ALB|      8|\n",
      "|            Algeria|          DZ|         DZA|     12|\n",
      "|     American Samoa|          AS|         ASM|     16|\n",
      "|            Andorra|          AD|         AND|     20|\n",
      "|             Angola|          AO|         AGO|     24|\n",
      "|           Anguilla|          AI|         AIA|    660|\n",
      "|         Antarctica|          AQ|         ATA|     10|\n",
      "|Antigua and Barbuda|          AG|         ATG|     28|\n",
      "|          Argentina|          AR|         ARG|     32|\n",
      "|            Armenia|          AM|         ARM|     51|\n",
      "|              Aruba|          AW|         ABW|    533|\n",
      "|          Australia|          AU|         AUS|     36|\n",
      "|            Austria|          AT|         AUT|     40|\n",
      "|         Azerbaijan|          AZ|         AZE|     31|\n",
      "|      Bahamas (the)|          BS|         BHS|     44|\n",
      "|            Bahrain|          BH|         BHR|     48|\n",
      "|         Bangladesh|          BD|         BGD|     50|\n",
      "|           Barbados|          BB|         BRB|     52|\n",
      "|            Belarus|          BY|         BLR|    112|\n",
      "+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df466184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', ['Java', 'Scala'], {'hair': 'black', 'eye': 'brown'}),\n",
       " ('Michael', ['Spark', 'Java', None], {'hair': 'brown', 'eye': None}),\n",
       " ('Robert', ['CSharp', ''], {'hair': 'red', 'eye': ''}),\n",
       " ('Washington', None, None),\n",
       " ('Jefferson', ['1', '2'], {})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array data\n",
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})\n",
    "        ]\n",
    "\n",
    "arrayData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5d512e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- knownLanguages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+-------------------+--------------------+\n",
      "|      name|     knownLanguages|          properties|\n",
      "+----------+-------------------+--------------------+\n",
      "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n",
      "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
      "|Washington|               null|                null|\n",
      "| Jefferson|             [1, 2]|                  {}|\n",
      "+----------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df5 = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "df5.printSchema()\n",
    "df5.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b830a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SQL, first, \n",
    "# create a temporary table on DataFrame \n",
    "# using createOrReplaceTempView() function.\n",
    "\n",
    "df5.createOrReplaceTempView(\"PERSON_Lenguages\")  # previous DF\n",
    "df6 = spark.sql(\"SELECT * from PERSON_Lenguages\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6318fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- knownLanguages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+-------------------+--------------------+\n",
      "|      name|     knownLanguages|          properties|\n",
      "+----------+-------------------+--------------------+\n",
      "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n",
      "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
      "|Washington|               null|                null|\n",
      "| Jefferson|             [1, 2]|                  {}|\n",
      "+----------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.printSchema()\n",
    "df6.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40734f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new data\n",
    "\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df7 = spark.createDataFrame(data=data, schema = columns)\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c20a0c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using createOrReplaceTempView() function.\n",
    "\n",
    "df7.createOrReplaceTempView(\"PERSON_DATA\")  # previous DF\n",
    "df9 = spark.sql(\"SELECT * from PERSON_DATA\")\n",
    "df9.show()\n",
    "df9.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed91c0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+\n",
      "|      name|     knownLanguages|          properties|\n",
      "+----------+-------------------+--------------------+\n",
      "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n",
      "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
      "|Washington|               null|                null|\n",
      "| Jefferson|             [1, 2]|                  {}|\n",
      "+----------+-------------------+--------------------+\n",
      "\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# have two  temporary table on DataFrame \n",
    "df6.show()\n",
    "df9.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e75ce9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|     M|       3|\n",
      "|     F|       2|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "groupDF = spark.sql(\"SELECT gender, count(*) from PERSON_DATA group by gender\")\n",
    "groupDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "960ce287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|     knownLanguages|\n",
      "+-------------------+\n",
      "|      [Java, Scala]|\n",
      "|[Spark, Java, null]|\n",
      "|         [CSharp, ]|\n",
      "|               null|\n",
      "|             [1, 2]|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupDFL = spark.sql(\"SELECT  knownLanguages from PERSON_Lenguages\")\n",
    "groupDFL.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71b8c0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 49:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|     M|       3|\n",
      "|     F|       2|\n",
      "+------+--------+\n",
      "\n",
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- count(1): long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "groupDF.show()\n",
    "groupDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "661b193c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|     knownLanguages|\n",
      "+-------------------+\n",
      "|      [Java, Scala]|\n",
      "|[Spark, Java, null]|\n",
      "|         [CSharp, ]|\n",
      "|               null|\n",
      "|             [1, 2]|\n",
      "+-------------------+\n",
      "\n",
      "root\n",
      " |-- knownLanguages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupDFL.show()\n",
    "groupDFL.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6767bd7",
   "metadata": {},
   "source": [
    "Add months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82fb9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "author SparkByExamples.com\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkMonth').getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import col,expr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c032c26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2019-01-23', 1), ('2019-06-24', 2), ('2019-09-20', 3)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef571eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|      date|increment|  inc_date|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(data).toDF(\"date\",\"increment\") \\\n",
    "    .select(col(\"date\"),col(\"increment\"), \\\n",
    "      expr(\"add_months(to_date(date,'yyyy-MM-dd'),cast(increment as int))\").alias(\"inc_date\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb3153b",
   "metadata": {},
   "source": [
    "Add new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "929ef85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|  3000|\n",
      "|     Anna|    Rose|     F|  4100|\n",
      "|   Robert|Williams|     M|  6200|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [('James','Smith','M',3000),\n",
    "  ('Anna','Rose','F',4100),\n",
    "  ('Robert','Williams','M',6200), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a17fc38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "+---------+--------+------+------+-------------+\n",
      "|firstname|lastname|gender|salary|bonus_percent|\n",
      "+---------+--------+------+------+-------------+\n",
      "|    James|   Smith|     M|  3000|          0.3|\n",
      "|     Anna|    Rose|     F|  4100|          0.3|\n",
      "|   Robert|Williams|     M|  6200|          0.3|\n",
      "+---------+--------+------+------+-------------+\n",
      "\n",
      "+---------+--------+------+------+------------+\n",
      "|firstname|lastname|gender|salary|bonus_amount|\n",
      "+---------+--------+------+------+------------+\n",
      "|    James|   Smith|     M|  3000|       900.0|\n",
      "|     Anna|    Rose|     F|  4100|      1230.0|\n",
      "|   Robert|Williams|     M|  6200|      1860.0|\n",
      "+---------+--------+------+------+------------+\n",
      "\n",
      "+---------+--------+------+------+---------------+\n",
      "|firstname|lastname|gender|salary|           name|\n",
      "+---------+--------+------+------+---------------+\n",
      "|    James|   Smith|     M|  3000|    James,Smith|\n",
      "|     Anna|    Rose|     F|  4100|      Anna,Rose|\n",
      "|   Robert|Williams|     M|  6200|Robert,Williams|\n",
      "+---------+--------+------+------+---------------+\n",
      "\n",
      "+---------+--------+------+------+------------+\n",
      "|firstname|lastname|gender|salary|current_date|\n",
      "+---------+--------+------+------+------------+\n",
      "|    James|   Smith|     M|  3000|  2023-01-19|\n",
      "|     Anna|    Rose|     F|  4100|  2023-01-19|\n",
      "|   Robert|Williams|     M|  6200|  2023-01-19|\n",
      "+---------+--------+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'salary1' not in df.columns:\n",
    "    print(\"aa\")\n",
    "    \n",
    "# Add new constanct column\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.withColumn(\"bonus_percent\", lit(0.3)) \\\n",
    "  .show()\n",
    "  \n",
    "#Add column from existing column\n",
    "df.withColumn(\"bonus_amount\", df.salary*0.3) \\\n",
    "  .show()\n",
    "\n",
    "#Add column by concatinating existing columns\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "df.withColumn(\"name\", concat_ws(\",\",\"firstname\",'lastname')) \\\n",
    "  .show()\n",
    "\n",
    "\n",
    "#Add current date\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "df.withColumn(\"current_date\", current_date()) \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fcd175e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+-----+\n",
      "|firstname|lastname|gender|salary|grade|\n",
      "+---------+--------+------+------+-----+\n",
      "|    James|   Smith|     M|  3000|    A|\n",
      "|     Anna|    Rose|     F|  4100|    B|\n",
      "|   Robert|Williams|     M|  6200|    C|\n",
      "+---------+--------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df.withColumn(\"grade\", \\\n",
    "   when((df.salary < 4000), lit(\"A\")) \\\n",
    "     .when((df.salary >= 4000) & (df.salary <= 5000), lit(\"B\")) \\\n",
    "     .otherwise(lit(\"C\")) \\\n",
    "  ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26f7216a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+\n",
      "|firstname|salary|bonus|\n",
      "+---------+------+-----+\n",
      "|    James|  3000|  0.3|\n",
      "|     Anna|  4100|  0.3|\n",
      "|   Robert|  6200|  0.3|\n",
      "+---------+------+-----+\n",
      "\n",
      "+---------+------+------------+\n",
      "|firstname|salary|bonus_amount|\n",
      "+---------+------+------------+\n",
      "|    James|  3000|       900.0|\n",
      "|     Anna|  4100|      1230.0|\n",
      "|   Robert|  6200|      1860.0|\n",
      "+---------+------+------------+\n",
      "\n",
      "+---------+------+----------+\n",
      "|firstname|salary|today_date|\n",
      "+---------+------+----------+\n",
      "|    James|  3000|2023-01-19|\n",
      "|     Anna|  4100|2023-01-19|\n",
      "|   Robert|  6200|2023-01-19|\n",
      "+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Add column using select\n",
    "df.select(\"firstname\",\"salary\", lit(0.3).alias(\"bonus\")).show()\n",
    "df.select(\"firstname\",\"salary\", lit(df.salary * 0.3).alias(\"bonus_amount\")).show()\n",
    "df.select(\"firstname\",\"salary\", current_date().alias(\"today_date\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2855e9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+\n",
      "|firstname|salary|bonus|\n",
      "+---------+------+-----+\n",
      "|    James|  3000|  0.3|\n",
      "|     Anna|  4100|  0.3|\n",
      "|   Robert|  6200|  0.3|\n",
      "+---------+------+-----+\n",
      "\n",
      "+---------+------+------------+\n",
      "|firstname|salary|bonus_amount|\n",
      "+---------+------+------------+\n",
      "|    James|  3000|       900.0|\n",
      "|     Anna|  4100|      1230.0|\n",
      "|   Robert|  6200|      1860.0|\n",
      "+---------+------+------------+\n",
      "\n",
      "+---------+------+----------+\n",
      "|firstname|salary|today_date|\n",
      "+---------+------+----------+\n",
      "|    James|  3000|2023-01-19|\n",
      "|     Anna|  4100|2023-01-19|\n",
      "|   Robert|  6200|2023-01-19|\n",
      "+---------+------+----------+\n",
      "\n",
      "+---------+------+-----+\n",
      "|firstname|salary|grade|\n",
      "+---------+------+-----+\n",
      "|    James|  3000|    B|\n",
      "|     Anna|  4100|    B|\n",
      "|   Robert|  6200|    B|\n",
      "+---------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Add columns using SQL\n",
    "df.createOrReplaceTempView(\"PER\") # temporary table\n",
    "\n",
    "spark.sql(\"select firstname,salary, '0.3' as bonus from PER\").show()\n",
    "spark.sql(\"select firstname,salary, salary * 0.3 as bonus_amount from PER\").show()\n",
    "spark.sql(\"select firstname,salary, current_date() as today_date from PER\").show()\n",
    "spark.sql(\"select firstname,salary, \" +\n",
    "          \"case salary when salary < 4000 then 'A' \"+\n",
    "          \"else 'B' END as grade from PER\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37cf7089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|  3000|\n",
      "|     Anna|    Rose|     F|  4100|\n",
      "|   Robert|Williams|     M|  6200|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from PER\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd2eeab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "+---------+--------+------+------+-------------+\n",
      "|firstname|lastname|gender|salary|bonus_percent|\n",
      "+---------+--------+------+------+-------------+\n",
      "|    James|   Smith|     M|  3000|          0.3|\n",
      "|     Anna|    Rose|     F|  4100|          0.3|\n",
      "|   Robert|Williams|     M|  6200|          0.3|\n",
      "+---------+--------+------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'salary1' not in df.columns:\n",
    "    print(\"aa\")\n",
    "    \n",
    "# Add new constanct column\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "dfM = df.withColumn(\"bonus_percent\", lit(0.3))\n",
    "dfM.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "153e744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+-------------+------------+\n",
      "|firstname|lastname|gender|salary|bonus_percent|bonus_amount|\n",
      "+---------+--------+------+------+-------------+------------+\n",
      "|    James|   Smith|     M|  3000|          0.3|       900.0|\n",
      "|     Anna|    Rose|     F|  4100|          0.3|      1230.0|\n",
      "|   Robert|Williams|     M|  6200|          0.3|      1860.0|\n",
      "+---------+--------+------+------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Add column from existing column\n",
    "dfM = dfM.withColumn(\"bonus_amount\", df.salary*0.3)\n",
    "dfM.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d30f451a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+-------------+------------+---------------+------------+\n",
      "|firstname|lastname|gender|salary|bonus_percent|bonus_amount|           name|current_date|\n",
      "+---------+--------+------+------+-------------+------------+---------------+------------+\n",
      "|    James|   Smith|     M|  3000|          0.3|       900.0|    James,Smith|  2023-01-19|\n",
      "|     Anna|    Rose|     F|  4100|          0.3|      1230.0|      Anna,Rose|  2023-01-19|\n",
      "|   Robert|Williams|     M|  6200|          0.3|      1860.0|Robert,Williams|  2023-01-19|\n",
      "+---------+--------+------+------+-------------+------------+---------------+------------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- bonus_percent: double (nullable = false)\n",
      " |-- bonus_amount: double (nullable = true)\n",
      " |-- name: string (nullable = false)\n",
      " |-- current_date: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Add column by concatinating existing columns\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "dfM = dfM.withColumn(\"name\", concat_ws(\",\",\"firstname\",'lastname')) \n",
    "\n",
    "\n",
    "#Add current date\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "dfM = dfM.withColumn(\"current_date\", current_date())\n",
    "\n",
    "dfM.show()\n",
    "dfM.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f32e8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+-------------+------------+---------------+------------+-----+\n",
      "|firstname|lastname|gender|salary|bonus_percent|bonus_amount|           name|current_date|grade|\n",
      "+---------+--------+------+------+-------------+------------+---------------+------------+-----+\n",
      "|    James|   Smith|     M|  3000|          0.3|       900.0|    James,Smith|  2023-01-19|    A|\n",
      "|     Anna|    Rose|     F|  4100|          0.3|      1230.0|      Anna,Rose|  2023-01-19|    B|\n",
      "|   Robert|Williams|     M|  6200|          0.3|      1860.0|Robert,Williams|  2023-01-19|    C|\n",
      "+---------+--------+------+------+-------------+------------+---------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "dfM = dfM.withColumn(\"grade\", \\\n",
    "   when((df.salary < 4000), lit(\"A\")) \\\n",
    "     .when((df.salary >= 4000) & (df.salary <= 5000), lit(\"B\")) \\\n",
    "     .otherwise(lit(\"C\")) \\\n",
    "  )\n",
    "\n",
    "dfM.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69cc7e",
   "metadata": {},
   "source": [
    "Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a02cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfM = dfM.drop('firstname')\n",
    "dfM = dfM.drop('lastname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "340432bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+------------+---------------+------------+-----+\n",
      "|gender|salary|bonus_percent|bonus_amount|           name|current_date|grade|\n",
      "+------+------+-------------+------------+---------------+------------+-----+\n",
      "|     M|  3000|          0.3|       900.0|    James,Smith|  2023-01-19|    A|\n",
      "|     F|  4100|          0.3|      1230.0|      Anna,Rose|  2023-01-19|    B|\n",
      "|     M|  6200|          0.3|      1860.0|Robert,Williams|  2023-01-19|    C|\n",
      "+------+------+-------------+------------+---------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfM.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f43ffd4",
   "metadata": {},
   "source": [
    "Aggregate functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a49888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        James|     Sales|  3000|\n",
      "|      Michael|     Sales|  4600|\n",
      "|       Robert|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|        James|     Sales|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Saif|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import pyspark\n",
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
    "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
    "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
    "from pyspark.sql.functions import variance,var_samp,  var_pop\n",
    "\n",
    "#spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "  \n",
    "  \n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "#df.show(truncate=False)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1cc9fbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx_count_distinct: 6\n",
      "avg: 3400.0\n",
      "+------------------------------------------------------------+\n",
      "|collect_list(salary)                                        |\n",
      "+------------------------------------------------------------+\n",
      "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
      "+------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------+\n",
      "|collect_set(salary)                 |\n",
      "+------------------------------------+\n",
      "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"approx_count_distinct: \" + \\\n",
    "      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n",
    "\n",
    "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n",
    "\n",
    "df.select(collect_list(\"salary\")).show(truncate=False) # all\n",
    "\n",
    "df.select(collect_set(\"salary\")).show(truncate=False) # distint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37c1e946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|count(DISTINCT department, salary)|\n",
      "+----------------------------------+\n",
      "|8                                 |\n",
      "+----------------------------------+\n",
      "\n",
      "Distinct Count of Department &amp; Salary: [Row(count(DISTINCT department, salary)=8)]\n",
      "count: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
    "df2.show(truncate=False)\n",
    "\n",
    "print(\"Distinct Count of Department &amp; Salary: \"+str(df2.collect()))\n",
    "\n",
    "print(\"count: \"+str(df.select(count(\"salary\")).collect()[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21966cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|first(salary)|\n",
      "+-------------+\n",
      "|3000         |\n",
      "+-------------+\n",
      "\n",
      "+------------+\n",
      "|last(salary)|\n",
      "+------------+\n",
      "|4100        |\n",
      "+------------+\n",
      "\n",
      "+-------------------+\n",
      "|kurtosis(salary)   |\n",
      "+-------------------+\n",
      "|-0.6467803030303032|\n",
      "+-------------------+\n",
      "\n",
      "+-----------+\n",
      "|max(salary)|\n",
      "+-----------+\n",
      "|4600       |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|min(salary)|\n",
      "+-----------+\n",
      "|2000       |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|3400.0     |\n",
      "+-----------+\n",
      "\n",
      "+--------------------+\n",
      "|skewness(salary)    |\n",
      "+--------------------+\n",
      "|-0.12041791181069571|\n",
      "+--------------------+\n",
      "\n",
      "+-------------------+-------------------+------------------+\n",
      "|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n",
      "+-------------------+-------------------+------------------+\n",
      "|765.9416862050705  |765.9416862050705  |726.636084983398  |\n",
      "+-------------------+-------------------+------------------+\n",
      "\n",
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|34000      |\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nestor/opt/anaconda3/envs/PySpark/lib/python3.9/site-packages/pyspark/sql/functions.py:214: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|sum(DISTINCT salary)|\n",
      "+--------------------+\n",
      "|20900               |\n",
      "+--------------------+\n",
      "\n",
      "+-----------------+-----------------+---------------+\n",
      "|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n",
      "+-----------------+-----------------+---------------+\n",
      "|586666.6666666666|586666.6666666666|528000.0       |\n",
      "+-----------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(first(\"salary\")).show(truncate=False)\n",
    "df.select(last(\"salary\")).show(truncate=False)\n",
    "df.select(kurtosis(\"salary\")).show(truncate=False)\n",
    "df.select(max(\"salary\")).show(truncate=False)\n",
    "df.select(min(\"salary\")).show(truncate=False)\n",
    "df.select(mean(\"salary\")).show(truncate=False)\n",
    "df.select(skewness(\"salary\")).show(truncate=False)\n",
    "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n",
    "    stddev_pop(\"salary\")).show(truncate=False)\n",
    "df.select(sum(\"salary\")).show(truncate=False)\n",
    "df.select(sumDistinct(\"salary\")).show(truncate=False)\n",
    "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab1bca",
   "metadata": {},
   "source": [
    "Order by - Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "50145799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NV   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |DE   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |NV   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NJ   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "author SparkByExamples.com\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,sum,avg,max\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NV\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"DE\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"NV\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NJ\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4dbecdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|      Michael|     Sales|   NV| 86000| 56|20000|\n",
      "|         Jeff| Marketing|   NV| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NJ| 91000| 50|21000|\n",
      "|        Raman|   Finance|   DE| 99000| 40|24000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSort=df.sort(df.state,df.salary, ascending=False)\n",
    "\n",
    "dfSort.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b267520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|state|sum(salary)|\n",
      "+-----+-----------+\n",
      "|   NY|     252000|\n",
      "|   NV|     166000|\n",
      "|   CA|     171000|\n",
      "|   DE|      99000|\n",
      "|   NJ|      91000|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSort=dfSort.groupBy(df.state).agg(sum(df.salary))\n",
    "dfSort.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265369be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
